{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/Users/carterhogan/CaseStudies/world_value_survey/analysis/data/wvs/WVS_Time_Series.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to now change political preference to a factored variable\n",
    "pol_pref = [\"Q240\"]\n",
    "\n",
    "# we need to remove all pol pref values less than 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.33333333, 1.        , 0.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline happiness average\n",
    "avg_hap = df['Q46'].mean()\n",
    "avg_hap\n",
    "\n",
    "unique_hap = df['Q46'].unique()\n",
    "unique_hap\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Filter for individuals with valid political preferences, happiness values, and income\n",
    "    merged_df = merged_df[(merged_df['Q240'] > 0) & (merged_df['Q46']>0) & (merged_df['Q288']>0)]\n",
    "    # Make political preference a dummy variable\n",
    "    pol_dummies = pd.get_dummies(merged_df['Q240'], prefix= \"pol_value\")\n",
    "    merged_df = pd.concat([merged_df,pol_dummies], axis = 1)\n",
    "    # Now find define average happiness and average income\n",
    "    avg_hap = merged_df['Q46'].mean()\n",
    "\n",
    "    merged_df = merged_df.assign(above_avg_hap  = merged_df['Q46']>=avg_hap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_income_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df[df['Q288']>0]\n",
    "    avg_inc = merged_df['Q288'].mean()\n",
    "\n",
    "    # define a two binary variables for income and happiness, where 1 represents above average and 0 below\n",
    "\n",
    "    df = merged_df.assign(above_avg_inc  = (df['Q288']>=avg_inc).astype(int))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_pol_pref(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  # Creating a pol pref index\n",
    "\n",
    "  Attaches the political preference of interviewees\n",
    "  Parameters:\n",
    "    df(pd.DataFrame): the wave7.csv dataframe stored by the data_filter_merge.ipynb\n",
    "  \n",
    "  Returns:\n",
    "    pd.DataFrame: Processed dataframe with the the political preference 'pol_pref'\n",
    "  \"\"\"\n",
    "\n",
    "  pol_pref = [\"Q240\"]\n",
    "  #ensure we only use valid observations of political preference\n",
    "  df = df[df['Q240'] > 0]\n",
    "  pol_dummies = pd.get_dummies(merged_df['Q240'], prefix= \"pol_value\")\n",
    "  df = pd.concat([df,pol_dummies], axis = 1)\n",
    "  \n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attach_happiness_index(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    #filter for valid happiness values\n",
    "    df = df[df['Q46']>0] \n",
    "    # save median happiness value\n",
    "    med_hap = df['Q46'].median()\n",
    "    # create a binary variable for those with above and below median happiness\n",
    "    df = df.assign(above_med_hap  = (df['Q46']>=med_hap).astype(int))\n",
    "    \n",
    "    hardships_questions = [f\"Q{i}\" for i in range(51, 56)]  # Composite index\n",
    "   \n",
    "\n",
    "    # Copy the DataFrame to work on\n",
    "    result = df.copy()\n",
    "\n",
    "    # 1. Convert 'Q56' to dummy variables -----------------------------------------\n",
    "    # Replace invalid values in Q56\n",
    "    result['Q56'] = result['Q56'].where(result['Q56'] > 0, pd.NA)\n",
    "\n",
    "    # country based imputation with mode\n",
    "    result['Q56'] = (\n",
    "        result.groupby('B_COUNTRY_ALPHA')['Q56']\n",
    "        .apply(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 0))\n",
    "        .reset_index(level=0, drop=True)  \n",
    "    )\n",
    "\n",
    "    # Generate dummy variables for Q56 with explicit integer type\n",
    "    dummies = pd.get_dummies(result['Q56'], prefix='standard_parents').astype(int)\n",
    "\n",
    "    # Generate dummy variables for Q56 with explicit integer type\n",
    "    dummies = pd.get_dummies(result['Q56'], prefix='standard_parents').astype(int)\n",
    "\n",
    "    # Rename the dummy columns\n",
    "    dummies.rename(columns={\n",
    "        'standard_parents_1.0': 'standard_parents_better',\n",
    "        'standard_parents_2.0': 'standard_parents_worse',\n",
    "        'standard_parents_3.0': 'standard_parents_same'\n",
    "    }, inplace=True)\n",
    "\n",
    "    dummies = dummies.drop(columns=['standard_parents_same'])\n",
    "\n",
    "    # Attach the dummy variables to the DataFrame\n",
    "    result = pd.concat([result, dummies], axis=1)\n",
    "\n",
    "    # 2. Impute missing values with the median -------------------------------------\n",
    "    # Create a median dictionary for countries\n",
    "    median_dict = {}\n",
    "    countries = result['B_COUNTRY_ALPHA'].unique()\n",
    "\n",
    "    # Reverse the scale for hardships questions\n",
    "    for hq in hardships_questions:\n",
    "        result[hq] = result[hq].where(result[hq] <= 0, 4 + 1 - result[hq])\n",
    "\n",
    "    for ct in countries:\n",
    "        median_dict[ct] = {}\n",
    "        for hq in hardships_questions:\n",
    "            # Calculate median for each question within each country\n",
    "            median_dict[ct][hq] = result.loc[(result[hq] > 0) & (result['B_COUNTRY_ALPHA'] == ct), hq].median()\n",
    "\n",
    "    # Populate the DataFrame with the imputed values\n",
    "    for hq in hardships_questions:\n",
    "        result[hq] = result.apply(\n",
    "            lambda row: median_dict[row['B_COUNTRY_ALPHA']][hq] if row[hq] <= 0 else row[hq], axis=1\n",
    "        )\n",
    "\n",
    "    # 3. Normalize happiness_questions using Min-Max Scaling ---------------------\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(result.loc[:, hardships_questions])\n",
    "    result.loc[:, hardships_questions] = scaler.transform(result.loc[:, hardships_questions])\n",
    "\n",
    "\n",
    "    # Composite variable: Hardships \n",
    "    result['hardships_questions'] = result.loc[:, hardships_questions].mean(axis=1)\n",
    "\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "world_value",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
